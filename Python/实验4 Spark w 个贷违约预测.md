# å®éªŒ4 Spark w/ ä¸ªè´·è¿çº¦é¢„æµ‹

## 191840265 å´å²ç¾¿

### 0. Sparkç¯å¢ƒé…ç½®

ä¸‡èƒ½çš„homebrewå€’äº†ï¼ğŸº

brew install sparkè£…äº†ä¸ªä¸çŸ¥é“æ˜¯ä»€ä¹ˆçš„ä¸œè¥¿ 7.2kb

![image-20211216103236961](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211216103236961.png)

ä¸ç®¡äº†ï¼Œè¿™æ¬¡å°±è€è€å®å®åˆ°é˜¿å¸•å¥‡å®˜ç½‘ä¸‹äº†ä¸ª

ç„¶å è§£å‹å®‰è£…

![image-20211216111418782](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211216111418782.png)

æˆ‘è¦èµç¾sparkï¼èµç¾sparkçš„è¯åœ¨æˆ‘çš„å¿ƒé‡Œï¼Œåœ¨æˆ‘çš„å”‡ä¸Šï¼

å•¥é…ç½®éƒ½ä¸è¦æ”¹å°±èƒ½å¼€

![image-20211216112449181](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211216112449181.png)

æˆ‘è¦æ”¶å›èµç¾sparkçš„è¯ï¼å› ä¸ºæˆ‘ç™½è£…äº†ï¼

![image-20211216113854087](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211216113854087.png)

pip installäº†ä¸€ä¸‹pyspark è¿˜åœ¨çº³é—·ä¸ºä»€ä¹ˆpysparkä¸ºä»€ä¹ˆè¿™ä¹ˆå¤§ï¼ˆï¼‰

æˆ‘è¿˜æ˜¯è¦èµç¾sparkï¼å› ä¸ºsparkæ”¯æŒpythonï¼

è‡³æ­¤ç¯å¢ƒå°± å¾ˆæ–¹ä¾¿çš„ï¼ŒèŠ±äº†ååˆ†é’Ÿçš„ï¼Œé…ç½®å®Œäº†ï¼ˆåœ¨è¿™é‡Œç»§ç»­è°´è´£hbaseä¸æ°¸è¿œæ‰¾ä¸åˆ°çš„regionserverï¼‰

### 1.ä»»åŠ¡ä¸€

æˆ‘è¦è°´è´£å¹¶å‘µæ–¥javaï¼Œä¸€ä¸ªä»»åŠ¡è€—çš„æ—¶é—´æ¯”æˆ‘åé¢ä¸¤ä¸ªä»»åŠ¡åŠ èµ·æ¥éƒ½å¤š

æŒ‰ç…§javaçš„åŠ£æ€§åŠ ä¸Šæˆ‘å¯¹javaä¸€ä¸ç‚¹ä¹Ÿä¸ç†Ÿï¼Œå¦‚æœæˆ‘è¦æŠŠcsvä¸­industryé‚£ä¸€åˆ—æå‡ºæ¥ï¼Œä¼°è®¡å¾—æŠ˜æˆ‘ä¸€å¤©æ€§å‘½

æ‰€ä»¥å¹²è„†ç›´æ¥pythonæŠŠæ•°æ®é¢„å¤„ç†å¥½äº†å–‚ç»™javaï¼š

~~~python
import pandas as pd
orig_df=pd.read_csv("train_data.csv")
new_df=orig_df['industry']
new_df.to_csv("CondemnJava.csv",index=False,header=False)
~~~

ä¸‹é¢å°±æ˜¯ä¸€ä¸ªç®€å•çš„WordCountäº†ï¼ŒæŒ‰ç…§job1:wordcountï¼Œjob2:æ’åºè¿›è¡Œï¼Œå¦‚ä¸‹ï¼š

~~~java
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;


import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.map.InverseMapper;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;


public class Loan_Count {
    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
        //å®ç°map()å‡½æ•°
        public void map(Object key, Text value, Context context)throws IOException,InterruptedException {
            String word = value.toString();
            context.write(new Text(word), new IntWritable(1));
        }
    }
    public static class IntSumReducer extends Reducer <Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();//å®ç°reduce()å‡½æ•°
        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;//éå†è¿­ä»£valuesï¼Œå¾—åˆ°åŒä¸€keyçš„æ‰€æœ‰value
            for (IntWritable val : values) { sum += val.get(); }
            result.set(sum);//äº§ç”Ÿè¾“å‡ºå¯¹<key, value>
            context.write(key, result);
        }
    }

    private static class IntWritableDecreasingComparator extends IntWritable.Comparator {
        public int compare(WritableComparable a, WritableComparable b) {
            //System.out.println("ss");
            return -super.compare(a, b);
        }
        public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
            //System.out.println("ss1");
            return -super.compare(b1, s1, l1, b2, s2, l2);
        }
    }

    public static void main(String[] args) throws Exception{
        Configuration config = new Configuration();
        Job job = Job.getInstance(config);
        job.setJobName("Loan_Count");
        job.setJarByClass(Loan_Count.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        job.setOutputFormatClass(SequenceFileOutputFormat.class);
        Path outPath = new Path("/user/quinton_541/tempDir");
        FileInputFormat.addInputPath(job, new Path("/user/quinton_541/input/"));
        FileOutputFormat.setOutputPath(job, outPath);
        job.waitForCompletion(true);
        Job sortjob = Job.getInstance(config);
        FileInputFormat.addInputPath(sortjob, outPath);
        sortjob.setOutputKeyClass(IntWritable.class);
        sortjob.setOutputValueClass(Text.class);
        sortjob.setInputFormatClass(SequenceFileInputFormat.class);
        sortjob.setMapperClass(InverseMapper.class);
        sortjob.setNumReduceTasks(1);
        sortjob.setSortComparatorClass(IntWritableDecreasingComparator.class);
        outPath= new Path("/user/quinton_541/Mission1_Output");
        FileOutputFormat.setOutputPath(sortjob,outPath);
        sortjob.waitForCompletion(true);
    }
}

~~~

ï¼ˆåæ¥æƒ³äº†ä¸‹å…¶å®ç›´æ¥ç”¨javaå¤„ç†æºcsvä¹Ÿæ²¡å•¥éš¾åº¦ï¼Œä½†æ˜¯è¿˜æ˜¯è¦å…ˆåœ¨å¤–é¢æŠŠç¬¬ä¸€è¡Œheaderå»æ‰ï¼Œmapperç±»æ”¹æˆè¿™æ ·

~~~java
public void map(Object key, Text value, Context context)throws IOException,InterruptedException {
            String line = value.toString();
  					String[] words = line.split(",");
            context.write(new Text(words[10]), new IntWritable(1));
        }
~~~

æ‰§è¡Œç»“æœå¦‚ä¸‹ï¼š

~~~shell
48216	é‡‘èä¸š
36048	ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š
30262	å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡
26954	ä½å®¿å’Œé¤é¥®ä¸š
24211	æ–‡åŒ–å’Œä½“è‚²ä¸š
24078	ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š
20788	å»ºç­‘ä¸š
17990	æˆ¿åœ°äº§ä¸š
15028	äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š
14793	é‡‡çŸ¿ä¸š
14758	å†œã€æ—ã€ç‰§ã€æ¸”ä¸š
9118	å›½é™…ç»„ç»‡
8892	æ‰¹å‘å’Œé›¶å”®ä¸š
8864	åˆ¶é€ ä¸š
~~~

ç”±äºvalueå’Œkeyå·²ç»åœ¨job2ä¸­çš„InverseMapperä¸­äº¤æ¢è¿‡äº†ï¼Œæ‰€ä»¥è¾“å‡ºå¥½åƒä¸å¤ªç¬¦åˆè¦æ±‚ã€‚

ç†è®ºä¸Šå†æ¥ä¸€ä¸ªjob3å†ç”¨ä¸€æ¬¡InverseMapperå°±å¯ä»¥æ¢å¤é¡ºåºäº†ï¼Œå¯è¿™å¤ªçƒ¦äº†ï¼Œä¸€ç‚¹ä¹Ÿä¸ä¼˜é›…ï¼

é‚£å°±å†æ¬¡è¯·å‡ºpythonå¤„ç†ä¸€ä¸‹å§ï¼ˆpyè´´è´´ï¼š

~~~py
f=open("Misson1_Output/part-r-00000",'r')
lines=f.readlines()
f_to_write=open("Mission1_Real_Output",'w')
for line in lines:
    key=line.split("\t")[1].strip()
    value=line.split("\t")[0]
    f_to_write.write("%s %s\n"%(key,value))
~~~

æœ€ç»ˆç»“æœ

```
é‡‘èä¸š 48216
ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š 36048
å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡ 30262
ä½å®¿å’Œé¤é¥®ä¸š 26954
æ–‡åŒ–å’Œä½“è‚²ä¸š 24211
ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š 24078
å»ºç­‘ä¸š 20788
æˆ¿åœ°äº§ä¸š 17990
äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š 15028
é‡‡çŸ¿ä¸š 14793
å†œã€æ—ã€ç‰§ã€æ¸”ä¸š 14758
å›½é™…ç»„ç»‡ 9118
æ‰¹å‘å’Œé›¶å”®ä¸š 8892
åˆ¶é€ ä¸š 8864
```

### 2. ä»»åŠ¡äºŒ

æˆ‘è¦èµç¾Sparkï¼Œèµç¾Sparkçš„æœ‰ç¦äº†ï¼

å› ä¸ºSparkæä¾›äº†pythonæ¥å£ï¼Œæ‰€ä»¥å¾ˆæ–¹ä¾¿çš„ï¼Œåœ¨äº†è§£äº†rddç›¸å…³æ“ä½œåå¾ˆå¿«å°±å†™å®Œäº†ä»»åŠ¡äºŒï¼ˆç§pythonæœ¬å½“ä¸Šæ‰‹ï¼‰

åŸºæœ¬æ€è·¯ï¼š

1.è¯»å–csvæ–‡ä»¶ä¸ºrdd

2.å‰”é™¤é¦–è¡Œçš„headers

3.æŒ‰é€—å·åˆ†å‰²æ¯ä¸€ä¸ªå¯¹è±¡ï¼Œæå–å‡ºæ¯ä¸€è¡Œä¸­â€œTotal_loanâ€

4.å°†total_loançš„å€¼è§„çº¦ä¸ºå…¶æ‰€åœ¨åŒºé—´çš„ä¸‹é™ï¼Œå³1541->1000ï¼Œ54541->54000ç­‰

5.æ‰§è¡Œç»Ÿè®¡ï¼ŒæŒ‰ç…§é”®å€¼æ’åºä¼šï¼Œå°†è§„çº¦åçš„åŒºé—´ä¸‹é™æ¢å¤ä¸ºåŒºé—´å¹¶è¾“å‡ºã€‚

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

~~~python
from pyspark import SparkContext
if __name__ == '__main__':
    sc=SparkContext.getOrCreate()
    # è¯»å–æ–‡ä»¶ä¸ºrddå¯¹è±¡
    lines=sc.textFile('train_data.csv')
    header = lines.first()  # ç¬¬ä¸€è¡Œ print(header)
    # å‰”é™¤csvçš„é¦–è¡Œ
    lines = lines.filter(lambda row: row != header)
    # æŒ‰é€—å·åˆ†å‰²åæå–ç¬¬ä¸‰ä¸ªå…ƒç´ ï¼štotal_loan
    lines=lines.map(lambda line:line.split(',')[2])
    # int((float(x)//1000)*1000) æå–å…ƒç´ æ‰€åœ¨åŒºé—´ä¸‹é™ï¼Œå³1541->1000ï¼Œ54541->54000
    counts = lines.map(lambda x: (int((float(x)//1000)*1000), 1)).reduceByKey(lambda a,b:a+b)
    #æ’åº
    counts=counts.sortByKey()
    output=counts.collect()
    #ç»Ÿè®¡å¥½åï¼Œå°†åŒºé—´ä¸‹é™è¿˜åŸä¸ºåŒºé—´ï¼š1000->(1000,2000)
    for (item, count) in output: print("(%i,%i),%i"%(item,item+1000,count))
    counts.saveAsTextFile("Misson2_Output")
~~~

è¿è¡Œç»“æœï¼š

~~~shell
(0,1000),2
(1000,2000),4043
(2000,3000),6341
(3000,4000),9317
(4000,5000),10071
(5000,6000),16514
(6000,7000),15961
(7000,8000),12789
(8000,9000),16384
(9000,10000),10458
(10000,11000),27170
(11000,12000),7472
(12000,13000),20513
(13000,14000),5928
(14000,15000),8888
(15000,16000),18612
(16000,17000),11277
(17000,18000),4388
(18000,19000),9342
(19000,20000),4077
(20000,21000),17612
(21000,22000),5507
(22000,23000),3544
(23000,24000),2308
(24000,25000),8660
(25000,26000),8813
(26000,27000),1604
(27000,28000),1645
(28000,29000),5203
(29000,30000),1144
(30000,31000),6864
(31000,32000),752
(32000,33000),1887
(33000,34000),865
(34000,35000),587
(35000,36000),11427
(36000,37000),364
(37000,38000),59
(38000,39000),85
(39000,40000),30
(40000,41000),1493
~~~

### 3.ä»»åŠ¡3

ä»»åŠ¡ä¸‰æ˜¯åˆ©ç”¨Spark SQLæ¥æ‰§è¡Œçš„ï¼Œå› æ­¤ç¬¬ä¸€æ­¥æ˜¯éœ€è¦å°†æ•°æ®è¯»å…¥sparkçš„dataframeï¼š

ï¼ˆsrdsï¼Œæœ€åè¿˜æ˜¯è½¬åŒ–æˆpddå•Šå‘¸rddï¼‰

~~~python
sc=SparkContext.getOrCreate()
spark=SQLContext(sc)
    #è¯»å–æ•°æ®è‡³sparkçš„dataframe
spark_df=spark.read.format("csv").option('header','true').option('inferScheme','true').load("train_data.csv")
~~~

ç”±äºæœ‰ç©ºå€¼ï¼Œå’Œæ ¼å¼ä¸Šçš„ä¸€äº›é—®é¢˜ï¼Œå¦‚æœå…ˆè¯»å…¥pandasçš„dfå†è¯»å…¥sparkçš„dfä¼šæŠ¥é”™ï¼š

![F1D6395FF5486A605066CBCE9BB0272F](/Users/quinton_541/Library/Containers/com.tencent.qq/Data/Library/Caches/Images/F1D6395FF5486A605066CBCE9BB0272F.jpg)

![AD0A296BD031E6C60786CA616FC6FB7B](/Users/quinton_541/Library/Containers/com.tencent.qq/Data/Library/Caches/Images/AD0A296BD031E6C60786CA616FC6FB7B.jpg)

#### 1.ç»Ÿè®¡æ‰€æœ‰ç”¨æˆ·æ‰€åœ¨å…¬å¸ç±»å‹ employer_type çš„æ•°é‡åˆ†å¸ƒå æ¯”æƒ…å†µã€‚

è¯´æ˜¯æ•°é‡åˆ†å¸ƒå æ¯”ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªwordcountï¼Œå› æ­¤é¦–å…ˆè¯»å–spark_dfä¸­çš„employer_typeåˆ—è½¬æ¢ä¸ºrddï¼Œå¹¶æ‰§è¡Œä¸€ä¸ªç®€å•çš„wordCountã€‚

æ³¨æ„ï¼Œåƒæˆ‘ä¸€æ ·å…ˆæå–employer_typeåˆ—ä½œä¸ºå­æ•°æ®æ¡†æ—¶ï¼Œåœ¨rdd.mapçš„æ—¶å€™è¿˜æ˜¯è¦å°†employer_typeçš„å€¼æå–å‡ºæ¥ã€‚ä»£ç å¦‚ä¸‹ï¼š

~~~python
		#æ‰§è¡Œä»»åŠ¡1:ç»Ÿè®¡æ‰€æœ‰ç”¨æˆ·æ‰€åœ¨å…¬å¸ç±»å‹ employer_type çš„æ•°é‡åˆ†å¸ƒå æ¯”æƒ…å†µã€‚
    #è¯»å–employer_typeä¸ºå•ç‹¬æ•°æ®æ¡†
    employer_type=spark_df.select("employer_type")
    #å°†employer_typeè½¬æ¢ä¸ºrddï¼Œå¹¶æå–æ¯ä¸€è¡Œå…·ä½“å¯¹è±¡çš„å€¼åè¿›è¡Œè¯é¢‘ç»Ÿè®¡
    counts=employer_type.rdd.map(lambda x:"{}".format(x.employer_type))\
    												.map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b)
    output=counts.collect()
    sum=0
    for (item,count) in output: sum+=count
    #å†™å…¥csv
    with open("Misson3.1_Output.csv", "w") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["å…¬å¸ç±»å‹","ç±»å‹å æ¯”"])
        for (item,count) in output:
            writer.writerow([item,count/sum])
~~~

ç»“æœå¦‚ä¸‹ï¼š

~~~shell
å…¬å¸ç±»å‹,ç±»å‹å æ¯”
ä¸–ç•Œäº”ç™¾å¼º,0.053706666666666666
ä¸Šå¸‚ä¼ä¸š,0.10012666666666667
æ”¿åºœæœºæ„,0.25815333333333335
å¹¼æ•™ä¸ä¸­å°å­¦æ ¡,0.09998333333333333
æ™®é€šä¼ä¸š,0.4543433333333333
é«˜ç­‰æ•™è‚²æœºæ„,0.03368666666666666
~~~

#### 2.ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·æœ€ç»ˆé¡»ç¼´çº³çš„åˆ©æ¯é‡‘é¢

æœ¬æ¥æƒ³ç€ç›´æ¥åœ¨dataframeé‡Œæ“ä½œçš„ï¼Œåæ¥è§‰å¾—è¿™æ˜¯ä¸ªmapreduceçš„æ´»ï¼Œå°±åˆè½¬æˆrddæ¥åšäº†0v0

æ€è·¯ï¼šæŠŠéœ€è¦çš„æ•°æ®"user_id","year_of_loan","monthly_payment","total_loan"ä»dfæå–å‡ºæ¥è½¬æ¢æˆrddï¼Œå¯¹äºrddä¸­æ¯ä¸ªå¯¹è±¡ï¼Œè®¡ç®—åæ•´ç†æˆï¼ˆuser_id,total_moneyï¼‰ï¼Œæå–åè¾“å‡ºã€‚

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

~~~python
#æ‰§è¡Œä»»åŠ¡2ï¼šç»Ÿè®¡æ¯ä¸ªç”¨æˆ·æœ€ç»ˆé¡»ç¼´çº³çš„åˆ©æ¯é‡‘é¢
    #åœ¨sparkæ•°æ®æ¡†ä¸­é€‰å–æ‰€éœ€æ•°æ®è‡³å­æ•°æ®æ¡†
	 total_money_para=spark_df.select("user_id","year_of_loan","monthly_payment","total_loan")
    #è½¬åŒ–ä¸ºrddåæŒ‰å…¬å¼è®¡ç®—
    res=total_money_para.rdd.map(lambda x:(x.user_id,float(x.year_of_loan)*float(x.monthly_payment)*12-float(x.total_loan)))
    output=res.collect()
    with open("Misson3.2_Output.csv", "w") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["user_id","total_money"])
        for (user_id,total_money) in output:
            writer.writerow([user_id,total_money])
~~~

éƒ¨åˆ†ç»“æœå¦‚ä¸‹ï¼š

~~~shell
user_id,total_money
0,3846.0
1,1840.6000000000004
2,10465.600000000002
3,1758.5200000000004
4,1056.880000000001
5,7234.639999999999
6,757.9200000000001
7,4186.959999999999
8,2030.7600000000002
9,378.72000000000116
10,4066.760000000002
11,1873.5599999999977
12,5692.279999999999
13,1258.6800000000003
14,6833.5999999999985
15,9248.200000000004
16,6197.119999999995
17,1312.4400000000005
18,5125.200000000001
19,1215.8400000000001
~~~



#### 3.ç»Ÿè®¡â¼¯å·¥ä½œå¹´é™ work_year è¶…è¿‡ 5 å¹´çš„ç”¨æˆ·çš„æˆ¿è´·æƒ…å†µ censor_status çš„æ•°é‡åˆ†å¸ƒå æ¯”æƒ…å†µ

è¿™ç©æ„æ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸ºwork_yearçš„æ ¼å¼ä¸æ˜¯ç®€å•çš„ä¸€ä¸ªæ•°å­—ï¼Œè€Œæ˜¯â€œ< 1 yearâ€,"x years"å’Œâ€œ10+ yearsâ€ï¼Œè¿˜æœ‰å¥½å¤šç©ºå€¼ï¼Œä¸ºäº†ç›´è§‚çœ‹å‡ºè¿™ä¸œè¥¿çš„ç»“æ„ï¼Œæˆ‘å…ˆå¯¹work_yearçš„é”®å€¼åšäº†ä¸€ä¸ªword_countç»Ÿè®¡ï¼Œç»“æœå¦‚ä¸‹ï¼š

~~~python
		work_year=spark_df.select("work_year")
    counts=work_year.rdd.map(lambda x:"{}".format(x.work_year))\
  											.map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b)
    output=counts.collect()
    for (a,b) in output: print(a,",",b)
~~~

~~~shell
4 years , 18044
10+ years , 98287
8 years , 13480
7 years , 13281
None , 17428
3 years , 23977
5 years , 19016
1 year , 19799
< 1 year , 24080
9 years , 11330
2 years , 27328
6 years , 13950
~~~

å› æ­¤ï¼Œä¸ºäº†æå–å‡ºwork_yearæ‰€å®é™…ä»£è¡¨çš„å·¥ä½œå¹´æ•°ï¼Œæˆ‘ä»¬åšå¦‚ä¸‹å¤„ç†ï¼š

1.å°†ç©ºå€¼è¡¥å……ä¸ºâ€0 yearâ€œï¼›2.Dataframeè½¬æ¢ä¸ºrdd; 3.å°†work_yearçš„å­—ç¬¦ä¸²æŒ‰ç©ºæ ¼åˆ†å‰²åæå–å€’æ•°ç¬¬äºŒé¡¹

è¿™æ ·ï¼Œwork_yearå°±å˜æˆäº†â€œxâ€ä¸â€œ10+â€ï¼Œæ»¡è¶³æ¡ä»¶çš„å³ä¸º"10+"ä¸å¤§äº5çš„ä¸ªä½æ•°

å°†work_yearçœ‹ä½œå­—ç¬¦ä¸²ï¼Œé‚£ä¹ˆå­—ç¬¦ä¸²é•¿åº¦>1ä¸å­—ç¬¦ä¸²é•¿åº¦ä¸º1å¹¶ä¸”è½¬åŒ–ä¸ºintåå¤§äº5çš„å³ä¸ºæ»¡è¶³æ¡ä»¶è€…

ç”¨rdd.filterå‡½æ•°æå–å³å¯ã€‚

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

~~~python
 		#æ‰§è¡Œä»»åŠ¡3ï¼šç»Ÿè®¡â¼¯å·¥ä½œå¹´é™ work_year è¶…è¿‡ 5 å¹´çš„ç”¨æˆ·çš„æˆ¿è´·æƒ…å†µ censor_status çš„æ•°é‡åˆ†å¸ƒå æ¯”æƒ…å†µ
    #åœ¨sparkæ•°æ®æ¡†ä¸­è¯»å–æ‰€éœ€æ•°æ®
    parameter=spark_df.select("user_id","censor_status","work_year")
    #work_year ç©ºå€¼å¡«è¡¥ä¸º"0 year"
    parameter=parameter.fillna("0 year")
    #work yearå½¢å¼ä¸º"<1 year","x years"ä¸"10+ year"ï¼ŒæŒ‰ç©ºæ ¼åˆ†å‰²åæå–å€’æ•°ç¬¬äºŒä½
    #æ»¡è¶³æ¡ä»¶çš„ä¸º"10+"ä¸å¤§äº5çš„ä¸ªä½æ•°
    res=parameter.rdd.map(lambda x:(x.user_id,x.censor_status,x.work_year.split(" ")[-2]))\
                                .filter(lambda x:((len(x[2])>1) or (len(x[2])==1 and int(x[2])>5)))
    output=res.collect()
    with open("Misson3.3_Output.csv", "w") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["user_id", "censor_status","work_year"])
        for (user_id, censor_status, work_year) in output:
            writer.writerow([user_id, censor_status, work_year])
~~~

éƒ¨åˆ†ç»“æœå¦‚ä¸‹ï¼š

~~~shell
user_id,censor_status,work_year
1,2,10+
2,1,10+
5,2,10+
6,0,8
7,2,10+
9,0,10+
10,2,10+
15,1,7
16,2,10+
17,0,10+
18,1,10+
20,1,7
21,2,10+
25,2,10+
26,0,10+
30,0,10+
31,0,6
33,1,10+
38,0,10+
39,1,10+
~~~

### 4.ä»»åŠ¡4

ä»»åŠ¡4æ˜¯ä¸€ä¸ªåˆ©ç”¨pyspark mlåº“å®ç°machine learningçš„ä»»åŠ¡ã€‚ä»æä¾›çš„æ•°æ®ç±»åˆ«æ¥çœ‹ï¼Œæˆ‘ä»¬çš„æ•°æ®æœ‰intä¸stringç±»å‹ï¼ˆç”±äºå¤ªå¤šäº†ï¼Œå°±ä¸åœ¨æ­¤ç½—åˆ—

æˆ‘ä»¬é¦–å…ˆå‰”é™¤ä¸ç»“æœæ— å…³çš„å› å­ï¼šloan_idä¸user_id

~~~PYTHON
spark_df = spark_df.drop('loan_id').drop('user_id')
~~~

å¹¶å°†æ‰€æœ‰çš„strç±»åˆ«çš„å› å­ï¼Œé€šè¿‡StringIndexerè½¬å˜ä¸ºIntå‹å˜é‡

~~~python
		StrLabel=["class","sub_class","work_type","employer_type","issue_date","industry","earlies_credit_mon","work_year"]
    for item in StrLabel:
        indexer=StringIndexer(inputCol=item,outputCol="%sIndex"%item)
        spark_df=indexer.fit(spark_df).transform(spark_df)
        spark_df=spark_df.drop(item)
~~~

åˆ°è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°½ç®¡ä¹‹å‰æ‰€æœ‰çš„ç±»åˆ«ï¼Œé•¿å¾—åƒInt/Doubleå‹ï¼Œä½†æ˜¯é€šè¿‡print(spark_df)æ¥çœ‹ï¼Œè¿™äº›ä¸è¿‡æ˜¯æŠ«ç€ç‹¼çš®çš„ç¾Šâ€”â€”è¿˜æ˜¯Stringç±»å‹ï¼š

~~~python
DataFrame[total_loan: string, year_of_loan: string, interest: string, monthly_payment: string, house_exist: string, house_loan_status: string, censor_status: string, marriage: string, offsprings: string, issue_date: string, use: string, post_code: string, region: string, debt_loan_ratio: string, del_in_18month: string, scoring_low: string, scoring_high: string, pub_dero_bankrup: string, early_return: string, early_return_amount: string, early_return_amount_3mon: string, recircle_b: string, recircle_u: string, initial_list_status: string, title: string, policy_code: string, f0: string, f1: string, f2: string, f3: string, f4: string, f5: string, is_default: string, classIndex: double, sub_classIndex: double, work_typeIndex: double, employer_typeIndex: double, industryIndex: double, earlies_credit_monIndex: double, work_yearIndex: double]
~~~

å› æ­¤é¦–å…ˆé€šè¿‡.cast(DoubleType)å…¨è½¬æ¢ä¸ºDoubleï¼š

~~~python
#å°†é•¿å¾—åƒDoubleä½†å®é™…ä¸Šè¿˜æ˜¯Stringçš„å˜é‡è½¬æ¢ä¸ºDouble
    for item in spark_df.columns:
        spark_df=spark_df.withColumn(item,spark_df[item].cast(typ.DoubleType()))
~~~

è‡³æ­¤ï¼Œæ•°æ®å¤„ç†å®Œæ¯•ï¼Œç°åœ¨æ•°æ®æ¡†å±•ç¤ºå¦‚ä¸‹ï¼ˆéƒ¨åˆ†ï¼‰ï¼š

![image-20211218194523722](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218194523722.png)

![image-20211218194721735](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218194721735.png)

æˆ‘ä»¬æå–é™¤äº†is_defaultå¤–çš„æ‰€æœ‰å‚æ•°å½¢æˆfeaturesåˆ—å‘é‡ï¼Œå¹¶å’Œis_defaultä¸€åŒæå–å‡ºæ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„æµ‹è¯•æ¨¡å‹ï¼š

~~~python
#å°†é™¤äº†is_defaultçš„featuresåˆ—è½¬æˆå‘é‡,å¹¶åŠ å…¥dfä¸­
    cols=spark_df.columns
    cols.remove("is_default")
    ass = VectorAssembler(inputCols=cols, outputCol="features")
    spark_df=ass.transform(spark_df)
    #é€‰å–featuresä¸labelå½¢æˆæ¸…æ™°çš„feature-labelæ¨¡å‹
    model_df = spark_df.select(["features","is_default"])
~~~

æŒ‰ç…§8:2çš„æ¯”ä¾‹æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œéšæœºæ•°ç§å­å–541:

~~~python
#æŒ‰8ï¼š2çš„æ¯”ä¾‹ï¼Œéšæœºæ•°ç§å­541 åˆ†å‰²è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    train_df,test_df=model_df.randomSplit([0.8,0.2],seed=541)
~~~

è‡³æ­¤ï¼Œæ¨¡å‹çš„æ•°æ®é€‰æ‹©ç»“æŸï¼Œä¸‹é¢å¼€å§‹å»ºç«‹æ¨¡å‹

##### è®­ç»ƒæ¨¡å‹é€‰å–ï¼š

ç”±äºæœ¬é¢˜æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªäºŒå…ƒç±»åˆ«è¯†åˆ«çš„é—®é¢˜ï¼Œæ¨¡å‹çš„é€‰å–ä¸è®­ç»ƒæ¨¡å‹çš„å»ºç«‹éƒ½æ¯”è¾ƒæ–¹ä¾¿ï¼ŒåŠ ä¹‹pyspark.ml.classificationæä¾›äº†å¤ªå¤šçš„åˆ†ç±»å™¨ä¾›é€‰æ‹©ï¼Œå› æ­¤éšä¾¿æ‰¾äº†å››ä¸ªï¼šéšæœºæ£®æ—ï¼ˆRandomForestï¼‰é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰å†³ç­–æ ‘ï¼ˆDecisionTree Classifierï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€‚

èµç¾Spark MLï¼ä»£ç ç®€æ´æ˜äº†ï¼š

~~~python
		#éšæœºæ£®æ—
    rf = cl.RandomForestClassifier(labelCol="is_default", numTrees=128, maxDepth=9).fit(train_df)
    res = rf.transform(test_df)
    rf_auc = ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("éšæœºæ£®æ—é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f"%rf_auc)
    #é€»è¾‘å›å½’
    log_reg = cl.LogisticRegression(labelCol='is_default').fit(train_df)
    res = log_reg.transform(test_df)
    log_reg_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("é€»è¾‘å›å½’é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % log_reg_auc)
    #å†³ç­–æ ‘
    DTC=cl.DecisionTreeClassifier(labelCol='is_default').fit(train_df)
    res=DTC.transform(test_df)
    DTC_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("å†³ç­–æ ‘é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % DTC_auc)
    #æ”¯æŒå‘é‡æœº
    SVM=cl.LinearSVC(labelCol='is_default').fit(train_df)
    res=SVM.transform(test_df)
    SVM_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("æ”¯æŒå‘é‡æœºé¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % SVM_auc)
~~~

ç»“æœå¦‚ä¸‹ï¼š

![image-20211218203728412](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218203728412.png)

æ›´æ”¹è®­ç»ƒæµ‹è¯•é›†ä¸º7:3ä¸6:4ï¼Œç»“æœå¦‚ä¸‹ï¼š

7:3:

![image-20211218204106655](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218204106655.png)

6:4

![image-20211218205137288](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218205137288.png)

æˆ‘ä»¬å‘ç°ï¼Œ

1.ç”±äºæ•°æ®é‡æ¯”è¾ƒå¤§ï¼ˆ30ä¸‡æ¡ï¼‰ï¼Œè®­ç»ƒé›†ä¸ç®¡æ˜¯å æ¯”ä¸º0.7æˆ–0.9ï¼Œè®­ç»ƒé‡éƒ½å¾ˆå¤§ï¼Œæ¨¡å‹å·²ç»æˆç†Ÿï¼Œå› æ­¤è®­ç»ƒ/æµ‹è¯•é›†åˆçš„å¤§å°åœ¨å‡†ç¡®ç‡ä¸Šçš„ååº”ä¸å¤§ï¼Œç”šè‡³å‡ºç°äº†è®­ç»ƒé›†è¶Šå°ï¼Œå‡†ç¡®ç‡åè€Œå˜å¤§çš„æƒ…å†µ

2.æ•´ä½“æ¥è¯´åœ¨æœ¬æ¬¡æ¨¡å‹ä¸­ï¼Œæ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡æŒ‰ä¸‹åˆ—æ’åºï¼š
$$
RF>Log\ Reg>SVM>Decision\ Tree
$$
è‡³äºä¸ºä»€ä¹ˆä¸ç”¨9:1...æ˜¯å› ä¸ºæç¤ºä¸‹åˆ—é”™è¯¯ï¼š

![image-20211218204348110](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218204348110.png)

![image-20211218204439942](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218204439942.png)

hmm...çœ‹èµ·æ¥å½“æ—¶ä¹°8Gçš„å†…å­˜ç¡®å®æ˜¯ä¸ªé”™è¯¯

![image-20211218204653412](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218204653412.png)

è§£å†³æ–¹æ¡ˆï¼šæ¢ç”µè„‘ï¼ˆï¼‰

å½“ç„¶ï¼Œä½œä¸ºä¸€ä¸ªæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œè¿™é‡Œçš„ä¼˜åŒ–ç©ºé—´å¤ªå¤§äº†ã€‚

å¯¹äºéšæœºæ£®æ—æ¥è¯´ï¼Œè¿™é‡Œçš„å‚æ•°è®¾ç½®å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ï¼Œä¼˜åŒ–ï¼š

![image-20211218205303564](/Users/quinton_541/Library/Application Support/typora-user-images/image-20211218205303564.png)

åŒæ—¶ï¼Œæä¾›çš„30wæ¡æ•°æ®ä¸­çš„å™ªéŸ³æ•°æ®ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥å¤„ç†ç­‰ç­‰ç­‰ç­‰

ä½†æ˜¯æ€»ä½“æ¥è¯´ï¼Œ85%çš„æ­£ç¡®ç‡å·²ç»æ˜¯éå¸¸å¯ä»¥æ¥å—çš„ä¸€ä¸ªç»“æœäº†ã€‚

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

~~~python
import pyspark.sql.types as typ
import pyspark.ml.classification as cl
import pyspark.ml.evaluation as ev
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark import SparkContext
from pyspark.sql import SQLContext
if __name__ == '__main__':
    sc = SparkContext.getOrCreate()
    spark = SQLContext(sc)
    spark_df = spark.read.format("csv").option('header', 'true').option('inferScheme', 'true').load("train_data.csv")
    #åˆ é™¤æ— å…³æ•°æ®
    spark_df = spark_df.drop('loan_id').drop('user_id')
    spark_df = spark_df.fillna("0")
    #å°†é•¿å¾—åƒstringçš„ç±»å‹å˜é‡é€šè¿‡æ˜ å°„è½¬æ¢ä¸ºDouble
    StrLabel=["class","sub_class","work_type","issue_date","employer_type","industry","earlies_credit_mon","work_year"]
    for item in StrLabel:
        indexer=StringIndexer(inputCol=item,outputCol="%sIndex"%item)
        spark_df=indexer.fit(spark_df).transform(spark_df)
        spark_df=spark_df.drop(item)
    #å°†é•¿å¾—åƒDoubleä½†å®é™…ä¸Šè¿˜æ˜¯Stringçš„å˜é‡è½¬æ¢ä¸ºDouble
    for item in spark_df.columns:
        spark_df=spark_df.withColumn(item,spark_df[item].cast(typ.DoubleType()))
    #å°†é™¤äº†is_defaultçš„featuresåˆ—è½¬æˆå‘é‡,å¹¶åŠ å…¥dfä¸­
    cols=spark_df.columns
    cols.remove("is_default")
    ass = VectorAssembler(inputCols=cols, outputCol="features")
    spark_df=ass.transform(spark_df)
    #é€‰å–featuresä¸labelå½¢æˆæ¸…æ™°çš„feature-labelæ¨¡å‹
    model_df = spark_df.select(["features","is_default"])
    #æŒ‰8ï¼š2çš„æ¯”ä¾‹ï¼Œéšæœºæ•°ç§å­541 åˆ†å‰²è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    train_df,test_df=model_df.randomSplit([0.8,0.2],seed=541)
    #éšæœºæ£®æ—
    rf = cl.RandomForestClassifier(labelCol="is_default", numTrees=128, maxDepth=9).fit(train_df)
    res = rf.transform(test_df)
    rf_auc = ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("éšæœºæ£®æ—é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f"%rf_auc)
    #é€»è¾‘å›å½’
    log_reg = cl.LogisticRegression(labelCol='is_default').fit(train_df)
    res = log_reg.transform(test_df)
    log_reg_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("é€»è¾‘å›å½’é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % log_reg_auc)
    #å†³ç­–æ ‘
    DTC=cl.DecisionTreeClassifier(labelCol='is_default').fit(train_df)
    res=DTC.transform(test_df)
    DTC_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("å†³ç­–æ ‘é¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % DTC_auc)
    #æ”¯æŒå‘é‡æœº
    SVM=cl.LinearSVC(labelCol='is_default').fit(train_df)
    res=SVM.transform(test_df)
    SVM_auc=ev.BinaryClassificationEvaluator(labelCol="is_default").evaluate(res)
    print("æ”¯æŒå‘é‡æœºé¢„æµ‹å‡†ç¡®ç‡ä¸ºï¼š%f" % SVM_auc)
~~~



### 5.æ€»ç»“

Sparkä½¿ç”¨èµ·æ¥è¿˜æ˜¯éå¸¸éå¸¸æ–¹ä¾¿çš„ï¼Œè‡³å°‘æœ€è€—æ—¶é—´çš„ç¯å¢ƒé…ç½®éƒ¨åˆ†ä¸ç”¨æµªè´¹å¤ªå¤šçš„æ—¶é—´ï¼ˆåœ¨è¿™é‡Œç»§ç»­è°´è´£hbaseä¸æ°¸è¿œæ‰¾ä¸åˆ°çš„regionserverï¼‰ï¼Œç”±äºæ”¯æŒpythonï¼Œåœ¨å†™ä»£ç çš„è¿‡ç¨‹ä¸­ä¹Ÿç›¸å¯¹æ˜¯æ¯”è¾ƒé¡ºåˆ©çš„ã€‚

å¯ä»¥è€ƒè™‘ä¸€ä¸‹åœ¨é‡‘å·¥é‡åŒ–ä¸Šçš„è¿ç”¨ï¼Œå¯¹äºGBã€TBçº§åˆ«çš„é«˜é¢‘äº¤æ˜“æ•°æ®ï¼ŒSparkå¯ä»¥ä¾¿æ·ï¼Œæœ‰æ•ˆçš„æé«˜ç¨‹åºæ‰§è¡Œæ•ˆç‡ï¼ˆçœŸæ­£çš„é‡‘èå¤§æ•°æ®ï¼‰
